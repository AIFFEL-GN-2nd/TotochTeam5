{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7f2f2c-6302-40e9-b643-4556a6604ed0",
   "metadata": {},
   "source": [
    "# 이웃집 토토치 파이토치 : Day 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34878d-54a2-40de-ae3a-75866a8e2097",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <p>📢 해당 게시물은 파이토치 공식 튜토리얼 중 <a href=\"https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\">예제로 배우는 파이토치(PYTORCH) </a>와 파이토치(PYTORCH) 기본 익히기-<a href=\"https://tutorials.pytorch.kr/beginner/basics/tensorqs_tutorial.html\">텐서(Tensor)</a>, <a href=\"https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html#id13\">DATASET과 DATALOADER</a>, <a href=\"https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html\">Autograd</a>를 재구성하여 작성되었습니다.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8956f1-a5de-4cfb-8c38-b2b4b098b2e5",
   "metadata": {},
   "source": [
    "#### 주요 키워드\n",
    "- Tensor\n",
    "- Autograd\n",
    "- Dataset과 Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd1b58-3ada-46cf-a2c9-655692e3a2a6",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### 목차\n",
    "1. [탠서(TENSOR)](#1.-텐서(TENSOR))\n",
    "    1. [텐서(tensor) 초기화](#A.-텐서(tensor)-초기화)\n",
    "    2. [텐서의 속성(Attribute)](#B.-텐서의-속성(Attribute))\n",
    "    3. [텐서 연산(Operation](#C.-텐서-연산(Operation))\n",
    "    4. [NumPy 변환(Bridge)](#D.-NumPy-변환(Bridge))\n",
    "    5. [[실습] Numpy to Tensor](#E.-[실습]-Numpy-to-Tensor)\n",
    "2. [Autograd](#2.-Autograd)\n",
    "    1. [Tensor, Function과 연산그래프(Computational graph)](#A.-Tensor,-Function과-연산그래프(Computational-graph))\n",
    "    2. [변화도(Gradient) 계산하기](#B.-변화도(Gradient)-계산하기)\n",
    "    3. [변화도 추적 멈추기](#C.-변화도-추적-멈추기)\n",
    "    4. [연산 그래프에 대한 추가 정보](#D.-연산-그래프에-대한-추가-정보)\n",
    "    5. [선택적으로 읽기(Optional Reading): 텐서 변화도와 야코비안 곱 (Jacobian Product)](E.-선택적으로-읽기(Optional-Reading):-텐서-변화도와-야코비안-곱(Jacobian-Product))\n",
    "    6. [[실습] Backpropagation with Autograd](#F.-[실습]-Backpropagation-with-Autograd)\n",
    "3. [DATASET과 DATALOADER](#3.-DATASET과-DATALOADER)\n",
    "    1. [데이터셋 불러오기](#A.-데이터셋-불러오기)\n",
    "    2. [데이터셋을 순회하고 시각화하기](#B.-데이터셋을-순회하고-시각화하기)\n",
    "    3. [파일에서 사용자 정의 데이터셋 만들기](#C.-파일에서-사용자-정의-데이터셋-만들기)\n",
    "    4. [DataLoader로 학습용 데이터 준비하기](#D.-DataLoader로-학습용-데이터-준비하기)\n",
    "    5. [DataLoader를 통해 순회하기](#E.-DataLoader를-통해-순회하기(iterate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cdc33-53ef-443e-967e-d221a4f8fbb5",
   "metadata": {},
   "source": [
    "## 1. 텐서(TENSOR) \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d8f21-377c-4aa0-941b-b7909cc2f2cf",
   "metadata": {},
   "source": [
    "텐서(tensor)는 배열(array)이나 행렬(matrix)과 매우 유사한 특수한 자료구조입니다.\n",
    "PyTorch에서는 텐서를 사용하여 모델의 입력(input)과 출력(output), 그리고 모델의 매개변수들을 부호화(encode)합니다.\n",
    "\n",
    "텐서는 GPU나 다른 하드웨어 가속기에서 실행할 수 있다는 점만 제외하면 [NumPy](https://numpy.org)의 ndarray와 유사합니다.\n",
    "실제로 텐서와 NumPy 배열(array)은 종종 동일한 내부(underly) 메모리를 공유할 수 있어 데이터를 복수할 필요가 없습니다. ([NumPy 변환(Bridge)](https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html#bridge-to-np-label) 참고)\n",
    "텐서는 또한 ([Autograd](autogradqs_tutorial.html)장에서 살펴볼) 자동 미분(automatic differentiation)에 최적화되어 있습니다.\n",
    "ndarray에 익숙하다면 Tensor API를 바로 사용할 수 있을 것입니다. 아니라면, 아래 내용을 함께 보시죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b21bbe-2e1f-4a0c-9863-cc7b9b4bf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33173f1c-e196-4f80-87f6-25adb3f7907e",
   "metadata": {},
   "source": [
    "### A. 텐서(tensor) 초기화\n",
    "텐서는 여러가지 방법으로 초기화할 수 있습니다. 다음 예를 살펴보세요:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02882d-b5c0-4f6a-979e-14484173f0f6",
   "metadata": {},
   "source": [
    "#### a. 데이터로부터 직접(directly) 생성하기\n",
    "데이터로부터 직접 텐서를 생성할 수 있습니다. 데이터의 자료형(data type)은 자동으로 유추합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80d93-e872-4cc8-9948-29e323d0cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcefc3-4830-4ad3-a74c-bd9e7976aa2f",
   "metadata": {},
   "source": [
    "#### b. NumPy 배열로부터 생성하기\n",
    "텐서는 NumPy 배열로 생성할 수 있습니다. (그 반대도 가능합니다 - [NumPy 변환(Bridge)](https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html#bridge-to-np-label) 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753be768-90dd-4806-9b63-dc9adec2736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd57ba-92e2-4ea9-8826-25196fdcb07d",
   "metadata": {},
   "source": [
    "#### c. 다른 텐서로부터 생성하기\n",
    "명시적으로 재정의(override)하지 않는다면, 인자로 주어진 텐서의 속성(모양(shape), 자료형(datatype))을 유지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100365e-0302-4ad2-8623-0b250116ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676896b-fa1e-46f2-afee-3e9da2457e0b",
   "metadata": {},
   "source": [
    "#### d. 무작위(random) 또는 상수(constant) 값을 사용하기\n",
    "`shape`은 텐서의 차원(dimension)을 나타내는 튜플(tuple)로, 아래 함수들에서는 출력 텐서의 차원을 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f0e06-3c50-4d4c-bd47-6d5b5810e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14c531-430a-45e7-818d-278876cc7735",
   "metadata": {},
   "source": [
    "### B. 텐서의 속성(Attribute)\n",
    "텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d18f60-2823-47ee-958f-b3de620f5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec2e3-b2e1-406c-a654-59cbc060aa4e",
   "metadata": {},
   "source": [
    "### C. 텐서 연산(Operation)\n",
    "전치(transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링(random sampling) 등, 100가지 이상의 텐서 연산들을 [여기](https://pytorch.org/docs/stable/torch.html)에서 확인할 수 있습니다.\n",
    "\n",
    "각 연산들은 (일반적으로 CPU보다 빠른) GPU에서 실행할 수 있습니다. Colab을 사용한다면, Edit > Notebook Settings 에서 GPU를 할당할 수 있습니다.\n",
    "\n",
    "기본적으로 텐서는 CPU에 생성됩니다. .to 메소드를 사용하면 (GPU의 가용성(availability)을 확인한 뒤) GPU로 텐서를 명시적으로 이동할 수 있습니다. 장치들 간에 큰 텐서들을 복사하는 것은 시간과 메모리 측면에서 비용이 많이든다는 것을 기억하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f8fb6-b044-4cbd-bcdc-80c1ec7a82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 존재하면 텐서를 이동합니다\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40866338-5f65-4d75-a97b-63457e90cff9",
   "metadata": {},
   "source": [
    "목록에서 몇몇 연산들을 시도해보세요. NumPy API에 익숙하다면 Tensor API를 사용하는 것은 식은 죽 먹기라는 것을 알게 되실 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fdca2-aab4-4e2d-b6bf-9062338d9101",
   "metadata": {},
   "source": [
    "#### a. NumPy식의 표준 인덱싱과 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e7a6a-6b64-45c0-ad27-99b8e26584e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b5a86-18ed-4af1-be4a-91717895d609",
   "metadata": {},
   "source": [
    "텐서 합치기 `torch.cat`을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있습니다. torch.cat 과 미묘하게 다른 또 다른 텐서 결합 연산인 [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)도 참고해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ef522-8161-4508-991f-32fd0db49ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8542e-a54a-40b8-982c-94723f164257",
   "metadata": {},
   "source": [
    "#### b. 산술 연산(Arithmetic operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431105fb-a68f-4d58-8550-03e0cae85986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다. y1, y2, y3은 모두 같은 값을 갖습니다.\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# 요소별 곱(element-wise product)을 계산합니다. z1, z2, z3는 모두 같은 값을 갖습니다.\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac408566-f305-4a2e-bfc6-6a4c979d4d11",
   "metadata": {},
   "source": [
    "**단일-요소(single-element) 텐서**의 모든 값을 하나로 집계(aggregate)하여 요소가 하나인 텐서의 경우, `item()`을 사용하여 Python 숫자 값으로 변환할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5ea67-3e56-4eb6-9234-cf081999185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096f157-2ca6-476f-9d2d-ed5add07fe2e",
   "metadata": {},
   "source": [
    "**바꿔치기(in-place) 연산** 연산 결과를 피연산자(operand)에 저장하는 연산을 바꿔치기 연산이라고 부르며,  `_` 접미사를 갖습니다. 예를 들어, x.copy_(y) 나 x.t_() 는 x 를 변경합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163fa49-91ea-42b1-8e5c-358b0dde0f3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>📌NOTE</b>\n",
    "    <div>바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있습니다. 따라서, 사용을 권장하지 않습니다.</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf19d2e-2adf-42ec-b312-e37c4c942710",
   "metadata": {},
   "source": [
    "### D. NumPy 변환(Bridge)\n",
    "CPU 상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1de7a-b49f-4b69-96e9-c4b4bc48731e",
   "metadata": {},
   "source": [
    "#### a. 텐서를 NumPy 배열로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0380fad1-29cc-42aa-99e8-08d324e4fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49599d36-5081-4c2c-91cd-2dcf2909dd35",
   "metadata": {},
   "source": [
    "텐서의 변경 사항이 NumPy 배열에 반영됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c53b2-9c6b-4db8-8503-ae7070281716",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be91ad-d832-438b-a538-037ccaead865",
   "metadata": {},
   "source": [
    "#### b. NumPy 배열을 텐서로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e1f5b-02d1-4aa6-8af5-ea52d60e7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc0d4d-8ff7-4b16-80af-1632f223aa9f",
   "metadata": {},
   "source": [
    "NumPy 배열의 변경 사항이 텐서에 반영됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c0529-b952-488d-8420-f4602c4e6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649cc4d-39c9-4676-aa96-ad39a9a4a84d",
   "metadata": {},
   "source": [
    "### E. [실습] Numpy to Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a591c-019f-4083-b5e0-a75ba5e09d6a",
   "metadata": {},
   "source": [
    "본질적으로, PyTorch에는 두가지 주요한 특징이 있습니다:\n",
    "\n",
    "- NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)\n",
    "- 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation)\n",
    "\n",
    "이 튜토리얼에서는 3차 다항식(third order polynomial)을 사용하여 $y=\\sin(x)$ 에 근사(fit)하는 문제를 다뤄보겠습니다. 신경망은 4개의 매개변수를 가지며, 정답과 신경망이 예측한 결과 사이의 유클리드 거리(Euclidean distance)를 최소화하여 임의의 값을 근사할 수 있도록 경사하강법(gradient descent)을 사용하여 학습하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb231346-9453-4fed-883a-2984bb42b219",
   "metadata": {},
   "source": [
    "#### a. Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09744d29-bdc0-4577-b8dc-e42e93888f6c",
   "metadata": {},
   "source": [
    "PyTorch를 소개하기 전에, 먼저 `NumPy`를 사용하여 신경망을 구성해보겠습니다.\n",
    "\n",
    "NumPy는 n-차원 배열 객체와 이러한 배열들을 조작하기 위한 다양한 함수들을 제공합니다. Num\n",
    "Py는 과학 분야의 연산을 위한 포괄적인 프레임워크(generic framework)입니다. \n",
    "\n",
    "NumPy는 연산 그래프(computation graph)나 딥러닝, 변화도(gradient)에 대해서는 알지 못합니다. 하지만 NumPy 연산을 사용하여 신경망의 순전파 단계와 역전파 단계를 직접 구현함으로써, 3차 다항식이 사인(sine) 함수에 근사하도록 만들 수 있습니다.\n",
    "\n",
    "✅ 구현해야 하는 사항 : Numpy를 이용한 순전파, 손실, 역전파\n",
    "- 이때, 입력값의 범위는 `-π ~ π` 이다. 따라서 구현되는 순전파 함수는 삼차다항식의 꼴이 된다.\n",
    "- 이때, 손실은 예측값과 실제값의 유클리드 거리로 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd4ac5-a369-435a-9e68-c475010d47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성합니다\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "print(f'------------------ init ------------------')\n",
    "print(f'x({len(x)}) : {x}')\n",
    "print(f'y({len(y)}) : {y}')\n",
    "print(f'가중치 초기값 : ')\n",
    "print(f'  a : {a}')\n",
    "print(f'  b : {b}')\n",
    "print(f'  c : {c}')\n",
    "print(f'  d : {d}')\n",
    "print(f'------------------------------------------')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(f'[t = {t+1:4d}] loss : {loss:.3f}')\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "print(f'------------------------------------------')\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca8ad5-edcd-4189-9d78-3a42d995fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(x, y, y_pred):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.plot(x, y, color='red', label=\"real\")\n",
    "    plt.plot(x, y_pred, color='blue', label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "show(x, y, a + b * x + c * x ** 2 + d * x ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f417d32-37d0-4773-9f65-3af636582c2d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><b>Q. `grad_y_pred`를 계산하는 수식에서 왜 2를 곱하는 이유는 무엇인가?</b></p>\n",
    "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b79538-48ca-473b-86a7-fa8841712c8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><b>Q. 각 가중치에 대한 gradient를 구하는 식은 어떻게 유도되었는가?</b></p>\n",
    "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2aa280-6127-41bf-a5e7-d91daa8c9ed8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><b>Q. 다음 함수들의 기능은 무엇인가?</b>\n",
    "    <a href=\"https://numpy.org/doc/stable/index.html\">hint</a></p>\n",
    "    <div>👉 numpy.linespace : (여기에 답을 입력해 주세요)<br>\n",
    "             👉 numpy.sin : (여기에 답을 입력해 주세요)<br>\n",
    "             👉 numpy.square : (여기에 답을 입력해 주세요)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c00bf-14f5-4975-8de7-52e86a9bb9ac",
   "metadata": {},
   "source": [
    "#### b. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25f57c-74f4-4989-a8c6-0d1e7b1b0c0d",
   "metadata": {},
   "source": [
    "NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없습니다. 현대의 심층 신경망에서 GPU는 종종 [50배 또는 그 이상](https://github.com/jcjohnson/cnn-benchmarks) 의 속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않습니다.\n",
    "\n",
    "이번에는 PyTorch의 가장 핵심적인 개념인 **텐서(Tensor)** 에 대해서 알아보겠습니다. PyTorch 텐서(Tensor)는 개념적으로 NumPy 배열과 동일합니다. \n",
    "- 텐서(Tensor)는 n-차원 배열이며, PyTorch는 이러한 텐서들의 연산을 위한 다양한 기능들을 제공합니다.\n",
    "- NumPy 배열처럼 PyTorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지 못하며, 과학적 분야의 연산을 위한 포괄적인 도구입니다. \n",
    "- 텐서는 연산 그래프와 변화도를 추적할 수도 있지만, 과학적 연산을 위한 일반적인 도구로도 유용합니다.\n",
    "- 또한 NumPy와는 다르게, PyTorch 텐서는 GPU를 사용하여 수치 연산을 가속할 수 있습니다. PyTorch 텐서를 GPU에서 실행하기 위해서는 단지 적절한 장치를 지정해주기만 하면 됩니다.\n",
    "\n",
    "위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계구현하되 입력/출력/가중치 값을 PyTorch 텐서를 사용하여 정의해 봅시다. \n",
    "\n",
    "✅ 구현해야 하는 사항 : 이전 구현에서 Numpy를 torch.Tensor로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c9266-1b2c-42cc-b405-ffe5b560ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성합니다\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "print(f'------------------ init ------------------')\n",
    "print(f'x({len(x)}, {x.__class__}) : {x}')\n",
    "print(f'y({len(y)}, {y.__class__}) : {y}')\n",
    "print(f'가중치 초기값 : ')\n",
    "print(f'  a({a.__class__}) : {a}')\n",
    "print(f'  b({b.__class__}) : {b}')\n",
    "print(f'  c({c.__class__}) : {c}')\n",
    "print(f'  d({d.__class__}) : {d}')\n",
    "print(f'------------------------------------------')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(f'[t = {t+1:4d}] loss : {loss:.3f}')\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'------------------------------------------')\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
    "\n",
    "show(x, y, a + b * x + c * x ** 2 + d * x ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ba389-11f0-406b-876c-87fa24bdb3be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <p><b>Q. 파이토치 Tensor가 Numpy에 비하여 가지는 장점이 무엇인가? 위의 문장들을 복사하지 말고 자신만의 표현으로 적어보자.</b></p>\n",
    "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e95c5e-71c5-4441-a558-dd1fd9f9e4bf",
   "metadata": {},
   "source": [
    "## 2. Autograd\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65780702-4ad4-444d-87d6-d0e34b8787b7",
   "metadata": {},
   "source": [
    "신경망을 학습할 때 가장 자주 사용되는 알고리즘은 **역전파* 입니다. 이 알고리즘에서, 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 **변화도(gradient)**에 따라 조정됩니다.\n",
    "\n",
    "이러한 변화도를 계산하기 위해 PyTorch에는 `torch.autograd`라고 불리는 자동 미분 엔진이 내장되어 있습니다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원합니다.\n",
    "\n",
    "입력 `x`, 매개변수 `w` 와 `b` , 그리고 일부 손실 함수가 있는 가장 간단한 단일 계층 신경망을 가정하겠습니다. PyTorch에서는 다음과 같이 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab002f5-707e-4e5b-8888-137a06eca942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa2e7d-b21c-4a6e-90c1-d67edcc28f72",
   "metadata": {},
   "source": [
    "### A. Tensor, Function과 연산그래프(Computational graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9705f65-99a2-4144-a6a5-79c971124881",
   "metadata": {},
   "source": [
    "이 코드는 다음의 **연산 그래프** 를 정의합니다:\n",
    "\n",
    "![](https://i.ibb.co/HFpgNcF/computational-graph.png)\n",
    "\n",
    "이 신경망에서, ``w`` 와 ``b`` 는 최적화를 해야 하는 **매개변수** 입니다. 따라서\n",
    "이러한 변수들에 대한 손실 함수의 변화도를 계산할 수 있어야 합니다. 이를 위해서 해당 텐서에\n",
    "``requires_grad`` 속성을 설정합니다.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>📌NOTE</b>\n",
    "    <div><b><i>requires_grad</i></b>의 값은 텐서를 생성할 때 설정하거나, 나중에 <b><i>x.requires_grad_(True)</i></b> 메소드를 사용하여 나중에 설정할 수도 있습니다.</div>\n",
    "</div>\n",
    "\n",
    "연산 그래프를 구성하기 위해 텐서에 적용하는 함수는 사실 ``Function`` 클래스의 객체입니다. 이 객체는 *순전파* 방향으로 함수를 계산하는 방법과, *역방향 전파* 단계에서 도함수(derivative)를 계산하는 방법을 알고 있습니다. 역방향 전파 함수에 대한 참조(reference)는 텐서의 ``grad_fn`` 속성에 저장됩니다. ``Function`` 에 대한 자세한 정보는 [이 문서](https://pytorch.org/docs/stable/autograd.html#function)에서 찾아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb880382-83f8-4eee-8dcc-f7a6ef936702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5658d1-39bb-4218-a057-8b9af5bf800b",
   "metadata": {},
   "source": [
    "### B. 변화도(Gradient) 계산하기\n",
    "신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 도함수(derivative)를 계산해야 합니다. 즉, ``x`` 와 ``y`` 의 일부 고정값에서 $\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$ 가 필요합니다.\n",
    "\n",
    "이러한 도함수를 계산하기 위해, ``loss.backward()`` 를 호출한 다음 ``w.grad`` 와\n",
    "``b.grad`` 에서 값을 가져옵니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704989c-91ff-4c8d-b2f4-7bb4540e76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af95bad-529b-4362-9a43-627fb600cbe3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>📌NOTE</b>\n",
    "    <div>\n",
    "        <p>• 연산 그래프의 잎(leaf) 노드들 중 <i><b>requires_grad</b></i> 속성이 <i><b>True</b></i> 로 설정된 노드들의 <i><b>grad</b></i> 속성만 구할 수 있습니다. 그래프의 다른 모든 노드에서는 변화도가 유효하지 않습니다.</p>\n",
    "        <p>• 성능 상의 이유로, 주어진 그래프에서의 <i><b>backward</b></i>를 사용한 변화도 계산은 한 번만 수행할 수 있습니다. 만약 동일한 그래프에서 여러번의 <i><b>backward</b></i> 호출이 필요하면, <i><b>backward</b></i> 호출 시에 <i><b>retrain_graph=True</b></i> 를 전달해야 합니다.</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbb790-2f37-44e7-86d7-86ff21351942",
   "metadata": {},
   "source": [
    "### C. 변화도 추적 멈추기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557d146-e7ee-4e42-8fb7-d20f807d22fd",
   "metadata": {},
   "source": [
    "기본적으로, `requires_grad=True`인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원합니다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파 연산만 필요한 경우에는, 이러한 추적이나 지원이 필요없을 수 있습니다. 연산 코드를 `torch.no_grad()` 블록으로 둘러싸서 연산 추적을 멈출 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1c43a-b570-4917-817a-e8003a5118da",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348b424-4424-4c42-8f29-2362a0724c1f",
   "metadata": {},
   "source": [
    "동일한 결과를 얻는 다른 방법은 텐서에 `detach()` 메소드를 사용하는 것입니다.\n",
    "\n",
    "변화도 추적을 멈춰야 하는 이유들은 다음과 같습니다:\n",
    "- 신경망의 일부 매개변수를 고정된 **매개변수(frozen parameter)**로 표시합니다. 이는 [사전 학습된 신경망을 미세조정](https://tutorials.pytorch.kr/beginner/finetuning_torchvision_models_tutorial.html) 할 때 매우 일반적인 시나리오입니다.\n",
    "- 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때 **연산 속도가 향상됩니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cedcf-8665-478f-ab0c-d7f7129e25fa",
   "metadata": {},
   "source": [
    "### D. 연산 그래프에 대한 추가 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca91d38-5c91-4dd0-8443-ce41423e5fbf",
   "metadata": {},
   "source": [
    "개념적으로, autograd는 데이터(텐서)의 및 실행된 모든 연산들(및 연산 결과가 새로운 텐서인 경우도 포함하여)의 기록을 [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) 객체로\n",
    "구성된 방향성 비순환 그래프(DAG; Directed Acyclic Graph)에 저장(keep)합니다. 이 방향성 비순환 그래프(DAG)의 잎(leave)은 입력 텐서이고, 뿌리(root)는 결과 텐서입니다. 이 그래프를 뿌리에서부터 잎까지 추적하면 연쇄 법칙(chain rule)에 따라 변화도를 자동으로 계산할 수 있습니다.\n",
    "\n",
    "순전파 단계에서, `autograd`는 다음 두 가지 작업을 동시에 수행합니다:\n",
    "\n",
    "- 요청된 연산을 수행하여 결과 텐서를 계산하고,\n",
    "- DAG에 연산의 *변화도 기능(gradient function)* 를 유지(maintain)합니다.\n",
    "\n",
    "역전파 단계는 DAG 뿌리(root)에서 ``.backward()`` 가 호출될 때 시작됩니다. ``autograd`` 는 이 때:\n",
    "\n",
    "- 각 ``.grad_fn`` 으로부터 변화도를 계산하고,\n",
    "- 각 텐서의 ``.grad`` 속성에 계산 결과를 쌓고(accumulate),\n",
    "- 연쇄 법칙을 사용하여, 모든 잎(leaf) 텐서들까지 전파(propagate)합니다.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>📌NOTE</b>\n",
    "    <div>\n",
    "  <b>PyTorch에서 DAG들은 동적(dynamic)입니다.</b><br>\n",
    "      주목해야 할 중요한 점은 그래프가 처음부터(from scratch) 다시 생성된다는 것입니다. 매번 <b><i>.bachward()</i></b> 가 호출되고 나면, autograd는 새로운 그래프를 채우기(populate) 시작합니다. 이러한 점 덕분에 모델에서 흐름 제어(control flow) 구문들을 사용할 수 있게 되는 것입니다; 매번 반복(iteration)할 때마다 필요하면 모양(shape)이나 크기(size), 연산(operation)을 바꿀 수 있습니다.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201244df-f973-48db-a7b2-8af96d91ad09",
   "metadata": {},
   "source": [
    "### E. 선택적으로 읽기(Optional Reading): 텐서 변화도와 야코비안 곱(Jacobian Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049c8c6-ba5f-40cf-a99c-bd301753053d",
   "metadata": {},
   "source": [
    "대부분의 경우, 스칼라 손실 함수를 가지고 일부 매개변수와 관련한 변화도를 계산해야 합니다. 그러나 출력 함수가 임의의 텐서인 경우가 있습니다. 이럴 때, PyTorch는 실제 변화도가 아닌 **야코비안 곱(Jacobian product)**\\ 을 계산합니다.\n",
    "\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ 이고, $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle`$ 일 때 벡터 함수 $\\vec{y}=f(\\vec{x})$에서 $\\vec{x}$에 대한 $\\vec{y}$의 변화도는 **야코비안 행렬(Jacobian matrix)**로 주어집니다:\n",
    "\n",
    "$$J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right) $$\n",
    "\n",
    "야코비안 행렬 자체를 계산하는 대신, PyTorch는 주어진 입력 벡터 $v=(v_1 \\dots v_m)$ 에 대한 **야코비안 곱(Jacobian Product)**  $v^T\\cdot J$ 을 계산합니다.\n",
    "이 과정은 $v$ 를 인자로 ``backward`` 를 호출하면 이뤄집니다. $v$의 크기는 곱(product)을 계산하려고 하는 원래 텐서의 크기와 같아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f36f8f-b5cc-4f06-a23a-a997a143102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b294856-bafd-4967-bd2e-608830bbdb57",
   "metadata": {},
   "source": [
    "동일한 인자로 `backward`를 두차례 호출하면 변화도 값이 달라집니다. 이는 역방향 전파를 수행할 때, PyTorch가 **변화도를 누적(accumulate)해두기 때문**입니다. 즉, 계산된 변화도의 값이 연산 그래프의 모든 잎(leaf) 노드의 `grad` 속성에 추가됩니다. 따라서 제대로된 변화도를 계산하기 위해서는 `grad` 속성을 먼저 0으로 만들어야 합니다. 실제 학습 과정에서는 *옵티마이저(optimizer)* 가 이 과정을 도와줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1d504-c110-46cf-a412-4307aa42529a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>📌NOTE</b>\n",
    "    <div>이전에는 매개변수 없이 <b><i>backward()</i></b> 함수를 호출했습니다. 이는 본질적으로 <b><i>backward(torch.tensor(1.0))</i></b>을 호출하는 것과 동일하며, 신경망 훈련 중의 손실과 같은 스칼라-값 함수의 변화도를 계산하는 유용한 방법입니다.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8699f-6d35-4efa-9e9a-f4660b6260c0",
   "metadata": {},
   "source": [
    "### F. [실습] Backpropagation with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acbc6d-9258-4629-9083-6084a6fb3108",
   "metadata": {},
   "source": [
    "앞선 실습에서는 신경망의 순전파 단계와 역전파 단계를 직접 구현해보았습니다. 작은 2계층(2-layer) 신경망에서는 역전파 단계를 직접 구현하는 것이 큰일이 아니지만, 복잡한 대규모 신경망에서는 매우 아슬아슬한 일일 것입니다.\n",
    "\n",
    "다행히도, [자동 미분](https://en.wikipedia.org/wiki/Automatic_differentiation) 을 사용하여 신경망의 역전파 단계 연산을 자동화할 수 있습니다. PyTorch의 **autograd** 패키지는 정확히 이런 기능을 제공합니다. Autograd를 사용하면, 신경망의 순전파 단계에서 **연산 그래프(computational graph)** 를 정의하게 됩니다; 이 그래프의 노드(node)는 텐서(tensor)이고, 엣지(edge)는 입력 텐서로부터 출력 텐서를 만들어내는 함수가 됩니다. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있습니다.\n",
    "\n",
    "이는 복잡하게 들리겠지만, 실제로 사용하는 것은 매우 간단합니다. 각 텐서는 연산그래프에서 노드로 표현됩니다. 만약 x 가 `x.requires_grad=True` 인 텐서라면 `x.grad` 어떤 스칼라 값에 대한 x 의 변화도를 갖는 또 다른 텐서입니다.\n",
    "\n",
    "이번 단계에서는 텐서 연산을 사용하여 순전파 단계를 계산하고, PyTorch autograd를 사용하여 변화도(gradient)를 계산해보겠습니다.\n",
    "\n",
    "✅ 구현해야 하는 사항 : 이전 구현에서 역전파 부분을 autograd 기능으로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94484464-1019-4631-aea7-c5c9b2fc662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
    "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를\n",
    "# 계산할 필요가 없음을 나타냅니다.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n",
    "# y = a + b x + c x^2 + d x^3\n",
    "# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가 \n",
    "# 있음을 나타냅니다.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "print(f'------------------ init ------------------')\n",
    "print(f'x({len(x)}) : {x}')\n",
    "print(f'y({len(y)}) : {y}')\n",
    "print(f'가중치 초기값 : ')\n",
    "print(f'  a : {a}')\n",
    "print(f'  b : {b}')\n",
    "print(f'  c : {c}')\n",
    "print(f'  d : {d}')\n",
    "print(f'------------------------------------------')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 텐서들 간의 연산을 사용하여 예측값 y를 계산합니다.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 텐서들간의 연산을 사용하여 손실(loss)을 계싼하고 출력합니다.\n",
    "    # 이 때 손실은 (1,) shape을 갖는 텐서입니다.\n",
    "    # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(f'[t = {t+1:4d}] loss : {loss:.3f}')\n",
    "\n",
    "    # autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는\n",
    "    # 모든 텐서들에 대한 손실의 변화도를 계산합니다. \n",
    "    # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를\n",
    "    # 갖는 텐서가 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 직접 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n",
    "    # autograd에서는 이를 추적하지 않을 것이기 때문입니다.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'------------------------------------------')\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
    "\n",
    "with torch.no_grad():\n",
    "    show(x, y, a + b * x + c * x ** 2 + d * x ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56adc9ff-6855-4f95-bea8-0e645905aef3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h4>Q. `loss.backward()`는 어떻게 역전파를 진행하는가?</h4>\n",
    "    <p>👉 (여기에 답을 입력해 주세요)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ede93-3555-4835-9dfe-dea552f1a2e6",
   "metadata": {},
   "source": [
    "## 3. DATASET과 DATALOADER\n",
    "---\n",
    "\n",
    "데이터 샘플을 처리하는 코드는 지저분(messy)하고 유지보수가 어려울 수 있습니다. 더 나은 가독성(readability)과 모듈성(modularity)을 위해 데이터셋 코드를 모델 학습 코드로부터 분리하는 것이 이상적입니다. PyTorch는 ``torch.utils.data.DataLoader``와 ``torch.utils.data.Dataset`` 의 두 가지 데이터 기본 요소를 제공하여 미리 준비해된(pre-loaded) 데이터셋 뿐만 아니라 가지고 있는 데이터를 사용할 수 있도록 합니다.\n",
    "``Dataset`` 은 샘플과 정답(label)을 저장하고, ``DataLoader`` 는 ``Dataset`` 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다.\n",
    "\n",
    "PyTorch의 도메인 특화 라이브러리들은 (FashionMNIST와 같은) 다양한 미리 준비해둔(pre-loaded) 데이터셋을 제공합니다. 데이터셋은 ``torch.utils.data.Dataset`` 의 하위 클래스로 개별 데이터를 특정하는 함수가 구현되어 있습니다. 이러한 데이터셋은 모델을 만들어보고(prototype) 성능을 측정(benchmark)하는데 사용할 수 있습니다.\n",
    "\n",
    "여기에서 데이터셋들을 찾아볼 수 있습니다:\n",
    "[이미지 데이터셋](https://pytorch.org/vision/stable/datasets.html), \n",
    "[텍스트 데이터셋](https://pytorch.org/text/stable/datasets.html) 및\n",
    "[오디오 데이터셋](https://pytorch.org/audio/stable/datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d517ee-96bf-4c97-8b2c-2dec58086070",
   "metadata": {},
   "source": [
    "### A. 데이터셋 불러오기\n",
    "\n",
    "`TorchVision` 에서 [Fashion-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist) 데이터셋을\n",
    "불러오는 예제를 살펴보겠습니다. Fashion-MNIST는 Zalando의 기사 이미지 데이터셋으로 60,000개의 학습 예제와 10,000개의 테스트 예제로 이루어져 있습니다. 각 예제는 흑백(grayscale)의 28x28 이미지와 10개 분류(class) 중 하나인 정답(label)으로 구성됩니다.\n",
    "\n",
    "다음 매개변수들을 사용하여 [FashionMNIST 데이터셋](https://pytorch.org/vision/stable/datasets.html#fashion-mnist)을 불러옵니다:\n",
    " - ``root`` 는 학습/테스트 데이터가 저장되는 경로입니다.\n",
    " - ``train`` 은 학습용 또는 테스트용 데이터셋 여부를 지정합니다.\n",
    " - ``download=True`` 는 ``root`` 에 데이터가 없는 경우 인터넷에서 다운로드합니다.\n",
    " - ``transform`` 과 ``target_transform`` 은 특징(feature)과 정답(label) 변형(transform)을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99015ed-d495-4fb6-902b-05f27aad92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d25b89-b98c-428a-b92a-9aafbf18909c",
   "metadata": {},
   "source": [
    "### B. 데이터셋을 순회하고 시각화하기\n",
    "\n",
    "Dataset 에 리스트(list)처럼 직접 접근(index)할 수 있습니다: training_data[index]. matplotlib 을 사용하여 학습 데이터의 일부를 시각화해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830ed75-b3ae-4244-9acd-b05f87a9fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75cdc7-65d2-4de5-b738-7e3ec44dd2a3",
   "metadata": {},
   "source": [
    "### C. 파일에서 사용자 정의 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e203219-14ba-44ed-821a-7fb375b34ea9",
   "metadata": {},
   "source": [
    "사용자 정의 Dataset 클래스는 반드시 3개 함수를 구현해야 합니다: `__init__`, `__len__`, and `__getitem__`. 아래 구현을 살펴보면 FashionMNIST 이미지들은 img_dir 디렉토리에 저장되고, 정답은 `annotations_file csv` 파일에 별도로 저장됩니다.\n",
    "\n",
    "다음 장에서 각 함수들에서 일어나는 일들을 자세히 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d3bcf-96a1-4073-9e72-239a57ba68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626548e7-caf4-4baf-9271-eecda95ffea6",
   "metadata": {},
   "source": [
    "#### a. \\_\\_init\\_\\_\n",
    "\n",
    "`__init__` 함수는 Dataset 객체가 생성(instantiate)될 때 한 번만 실행됩니다. 여기서는 이미지와 주석 파일(annotation_file)이 포함된 디렉토리와 (다음 장에서 자세히 살펴볼) 두가지 변형(transform)을 초기화합니다. \n",
    "\n",
    "labels.csv 파일은 다음과 같습니다: \n",
    "```\n",
    "tshirt1.jpg, 0\n",
    "tshirt2.jpg, 0\n",
    "......\n",
    "ankleboot999.jpg, 9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486171b8-cb30-4b3b-a4b1-4a4dca46698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e172b99-bcb3-4943-89f5-309a8a0d99e3",
   "metadata": {},
   "source": [
    "#### b. \\_\\_len\\_\\_\n",
    "\n",
    "`__len__` 함수는 데이터셋의 샘플 개수를 반환합니다.\n",
    "\n",
    "예:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6901271-9687-4c3a-9ea3-e6fc1fc6a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee14b1-ff9e-4089-adfc-8a853effab6f",
   "metadata": {},
   "source": [
    "#### c. \\_\\_getitem\\_\\_\n",
    "\n",
    "`__getitem__` 함수는 주어진 인덱스 ``idx`` 에 해당하는 샘플을 데이터셋에서 불러오고 반환합니다.<br>\n",
    "인덱스를 기반으로, 디스크에서 이미지의 위치를 식별하고, ``read_image`` 를 사용하여 이미지를 텐서로 변환하고, ``self.img_labels`` 의 csv 데이터로부터 해당하는 정답(label)을 가져오고, (해당하는 경우) 변형(transform) 함수들을 호출한 뒤, 텐서 이미지와 라벨을 Python 사전(dict)형으로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a24918-bc86-48d2-b503-993ecc6b6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    sample = {\"image\": image, \"label\": label}\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c936c9-fe4e-43ca-9b6f-f12e59ef01ce",
   "metadata": {},
   "source": [
    "### D. DataLoader로 학습용 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f026f35-f761-4d82-b9c2-7347a0a84113",
   "metadata": {},
   "source": [
    "`Dataset`은 데이터셋의 특징(feature)을 가져오고 하나의 샘플에 정답(label)을 지정하는 일을 한 번에 합니다. 모델을 학습할 때, 일반적으로 샘플들을 “미니배치(minibatch)”로 전달하고, 매 에폭(epoch)마다 데이터를 다시 섞어서 과적합(overfit)을 막고, Python의 multiprocessing 을 사용하여 데이터 검색 속도를 높이려고 합니다.\n",
    "\n",
    "`DataLoader`는 간단한 API로 이러한 복잡한 과정들을 추상화한 순회 가능한 객체(iteratable)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c985b57-d35e-4d41-8265-80246ff7bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426442b1-4eaf-422d-bc6a-77e8c4894094",
   "metadata": {},
   "source": [
    "### E. DataLoader를 통해 순회하기(iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da5074-94f1-4524-be21-bfefe3200d57",
   "metadata": {},
   "source": [
    "``DataLoader`` 에 데이터셋을 불러온 뒤에는 필요에 따라 데이터셋을 순회(iterate)할 수 있습니다.\n",
    "아래의 각 순회(iteration)는 (각각 ``batch_size=64`` 의 특징(feature)과 정답(label)을 포함하는) ``train_features`` 와\n",
    "``train_labels`` 의 묶음(batch)을 반환합니다. ``shuffle=True`` 로 지정했으므로, 모든 배치를 순회한 뒤 데이터가 섞입니다.\n",
    "(데이터 불러오기 순서를 보다 세밀하게(finer-grained) 제어하려면 [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)\n",
    "를 살펴보세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd90d9-5408-429e-b047-69e64e56c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 정답(label)을 표시합니다.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73734d03-4541-4dbe-a60c-bddd88eea320",
   "metadata": {},
   "source": [
    "## Ref\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b980e2e-bce5-49eb-b881-feeca2c6d31c",
   "metadata": {},
   "source": [
    "- Autograd\n",
    "    - [PyTorch Autograd: Understanding the heart of PyTorch’s magic](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)\n",
    "    - [PyTorch Autograd Explained - In-depth Tutorial](https://youtu.be/MswxJw-8PvE)\n",
    "    - [How Pytorch Backward() function works](https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
